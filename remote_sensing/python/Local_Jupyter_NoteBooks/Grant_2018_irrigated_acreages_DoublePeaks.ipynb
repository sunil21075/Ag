{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "# from shapely.geometry import Point, Polygon\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "import os, os.path\n",
    "\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "# from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import sys\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "sys.path.append('/home/hnoorazar/remote_sensing_codes/')\n",
    "\n",
    "import remote_sensing_core as rc\n",
    "import remote_sensing_core as rcp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = \"/Users/hn/Documents/01_research_data/remote_sensing/01_NDVI_TS/\" + \\\n",
    "            \"04_Irrigated_eastern_Cloud70/Grant_2018_irrigated/\" + \\\n",
    "            \"Grant_Irrigated_EVI_2018_NassIn_NotCorrectYears/\"\n",
    "\n",
    "data_base = \"/Users/hn/Documents/01_research_data/remote_sensing/\" + \\\n",
    "            \"01_NDVI_TS/04_Irrigated_eastern_Cloud70/Grant_2018_irrigated/\" + \\\n",
    "            \"savitzky_EVI/Grant_Irrigated_2018_no_plot/\"\n",
    "\n",
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\"\n",
    "\n",
    "double_crop_potens = pd.read_csv(param_dir + \"double_crop_potential_plants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sav_win_size, sav_order, delt\n",
    "parameters = [(3, 1, 0.1), (3, 1, 0.2), (3, 1, 0.3), (3, 1, 0.4),\n",
    "              (5, 1, 0.1), (5, 1, 0.2), (5, 1, 0.3), (5, 1, 0.4),\n",
    "              (7, 1, 0.1), (7, 1, 0.2), (7, 1, 0.3), (7, 1, 0.4),\n",
    "              (9, 1, 0.1), (9, 1, 0.2), (9, 1, 0.3), (9, 1, 0.4),\n",
    "\n",
    "              (3, 2, 0.1), (3, 2, 0.2), (3, 2, 0.3), (3, 2, 0.4),\n",
    "              (5, 2, 0.1), (5, 2, 0.2), (5, 2, 0.3), (5, 2, 0.4),\n",
    "              (7, 2, 0.1), (7, 2, 0.2), (7, 2, 0.3), (7, 2, 0.4),\n",
    "              (9, 2, 0.1), (9, 2, 0.2), (9, 2, 0.3), (9, 2, 0.4),\n",
    "\n",
    "              # (3, 3, 0.1), (3, 3, 0.2), (3, 3, 0.3), (3, 3, 0.4),\n",
    "              (5, 3, 0.1), (5, 3, 0.2), (5, 3, 0.3), (5, 3, 0.4),\n",
    "              (7, 3, 0.1), (7, 3, 0.2), (7, 3, 0.3), (7, 3, 0.4),\n",
    "              (9, 3, 0.1), (9, 3, 0.2), (9, 3, 0.3), (9, 3, 0.4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a data table for output.\n",
    "output_columns = ['params',\n",
    "                  'NassIn_AllFields_AllYears', \n",
    "                  'NassOut_AllFields_AllYears',\n",
    "                  'NassIn_AllFields_CorrectYear', \n",
    "                  'NassOut_AllFields_CorrectYear',\n",
    "                  \n",
    "                  'NassIn_DoublePoten_AllYears', \n",
    "                  'NassOut_DoublePoten_AllYears',\n",
    "                  'NassIn_DoublePoten_CorrectYear', \n",
    "                  'NassOut_DoublePoten_CorrectYear'\n",
    "                 ]\n",
    "\n",
    "output_df = pd.DataFrame(data=None,\n",
    "                         index=np.arange(len(parameters)), \n",
    "                         # index = parameters,\n",
    "                         columns = output_columns)\n",
    "output_df['params'] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num, param in enumerate(parameters):\n",
    "#     print(\"Parameter {}: {}\".format(num, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, param in enumerate(parameters):\n",
    "    Sav_win_size = param[0]\n",
    "    sav_order = param[1]\n",
    "    delt = param[2]\n",
    "    \n",
    "    data_dir = data_base + \"delta\" + str(delt) + \\\n",
    "               \"_Sav_win\" + str(Sav_win_size) + \"_Order\" + str(sav_order) + \"/\"\n",
    "    curr_table = pd.read_csv(data_dir + \"all_poly_and_maxs_savitzky.csv\", low_memory=False)\n",
    "    \n",
    "    #\n",
    "    # drop last empty row\n",
    "    #\n",
    "    curr_table.drop(curr_table.tail(1).index, inplace=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    The data table includes all maximum information. \n",
    "    So, each field is repeated several times.\n",
    "    We need to get unique fields.\n",
    "    \"\"\"\n",
    "    curr_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "    curr_table.drop_duplicates(inplace=True)\n",
    "    \n",
    "    ###\n",
    "    ###  Pick those with more than two peaks in them\n",
    "    ###\n",
    "    curr_table = curr_table[curr_table[\"max_count\"] >= 2]\n",
    "\n",
    "    curr_table[\"DataSrc\"] = curr_table[\"DataSrc\"].str.lower()\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    ###\n",
    "    ###  NassIn, AllFields, AllYears \n",
    "    ###  i.e. Everything other than those with \n",
    "    ###  no maximum detected on their time series.\n",
    "    ###\n",
    "    NassIn_AllFields_AllYears_Acr = np.sum(curr_table['ExctAcr'])\n",
    "    #     print (\"1) is NASS in? should be ...\" )\n",
    "    #     print (curr_table.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    ###\n",
    "    ###  NassOut_AllFields_AllYears\n",
    "    ###\n",
    "    NassOut_AllFields_AllYears = curr_table[curr_table.DataSrc != 'nass'].copy()\n",
    "\n",
    "    #     print (\"2) is NASS in? should NOT be ...\")\n",
    "    #     print (NassOut_AllFields_AllYears.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    NassOut_AllFields_AllYears_Acr = np.sum(NassOut_AllFields_AllYears['ExctAcr'])\n",
    "    del(NassOut_AllFields_AllYears)\n",
    "\n",
    "    ###\n",
    "    ###  NassIn AllFields CorrectYear\n",
    "    ###\n",
    "    NassIn_AllFields_CorrectYear = curr_table[curr_table[\"LstSrvD\"].str.contains(\"2018\", na=False)].copy()\n",
    "\n",
    "    #     print (\"3) is NASS in? should be ...\")\n",
    "    #     print ( NassIn_AllFields_CorrectYear.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    NassIn_AllFields_CorrectYear_Acr = np.sum(NassIn_AllFields_CorrectYear['ExctAcr'])\n",
    "    del(NassIn_AllFields_CorrectYear)\n",
    "\n",
    "    ###\n",
    "    ###  NassOut AllFields CorrectYear\n",
    "    ###\n",
    "    NassOut_AllFields = curr_table[curr_table.DataSrc != 'nass'].copy()\n",
    "    NassOut_AllFields_CorrectYear = \\\n",
    "                 NassOut_AllFields[NassOut_AllFields[\"LstSrvD\"].str.contains(\"2018\", na=False)].copy()\n",
    "    \n",
    "    #     print (\"4) is NASS in? should NOT be ...\")\n",
    "    #     print (NassOut_AllFields_CorrectYear.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    NassOut_AllFields_CorrectYear_Acr = np.sum(NassOut_AllFields_CorrectYear['ExctAcr'])\n",
    "    del(NassOut_AllFields, NassOut_AllFields_CorrectYear)\n",
    "\n",
    "    ###############################################################\n",
    "    #####\n",
    "    #####      double potentials\n",
    "    #####\n",
    "    ###############################################################\n",
    "\n",
    "    curr_double_poten = curr_table[curr_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "    del(curr_table)\n",
    "\n",
    "    ###\n",
    "    ###  NassIn, double potential, AllYears (i.e. Everything other than non-max)\n",
    "    ###\n",
    "    NassIn_DoublePoten_AllYears_Acr = np.sum(curr_double_poten['ExctAcr'])\n",
    "    \n",
    "    #     print (\"1) is NASS in? should be ...\")\n",
    "    #     print (curr_double_poten.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    ###\n",
    "    ###  NassOut, double potential, AllYears\n",
    "    ###\n",
    "    NassOut_DoublePoten_AllYears = curr_double_poten[curr_double_poten.DataSrc != 'nass'].copy()\n",
    "    NassOut_DoublePoten_AllYears_Acr = np.sum(NassOut_DoublePoten_AllYears['ExctAcr'])\n",
    "\n",
    "    #     print (\"2) is NASS in? should NOT be ...\") \n",
    "    #     print (NassOut_DoublePoten_AllYears.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    ###\n",
    "    ###  NassIn, double potential, CorrectYear\n",
    "    ###\n",
    "\n",
    "    NassIn_DoublePoten_CorrectYear = \\\n",
    "                curr_double_poten[curr_double_poten[\"LstSrvD\"].str.contains(\"2018\", na=False)].copy()\n",
    "    NassIn_DoublePoten_CorrectYear_Acr = np.sum(NassIn_DoublePoten_CorrectYear['ExctAcr'])\n",
    "    \n",
    "    #     print (\"3) is NASS in? should be ...\")\n",
    "    #     print (NassIn_DoublePoten_CorrectYear.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    del(NassIn_DoublePoten_CorrectYear)\n",
    "\n",
    "    ###\n",
    "    ###  NassOut, double potential, CorrectYear\n",
    "    ###\n",
    "\n",
    "    NassOut_DoublePoten = curr_double_poten[curr_double_poten.DataSrc != 'nass'].copy()\n",
    "    NassOut_DoublePoten_CorrectYear = \\\n",
    "                 NassOut_DoublePoten[NassOut_DoublePoten[\"LstSrvD\"].str.contains(\"2018\", na=False)].copy()\n",
    "\n",
    "    #     print (\"4) is NASS in? should NOT be ...\")\n",
    "    #     print (NassOut_DoublePoten_CorrectYear.DataSrc.unique())\n",
    "    #     print(\"_____________________________________________\")\n",
    "\n",
    "    NassOut_DoublePoten_CorrectYear_Acr = np.sum(NassOut_DoublePoten_CorrectYear['ExctAcr'])\n",
    "    del(NassOut_DoublePoten, NassOut_DoublePoten_CorrectYear)\n",
    "    \n",
    "    ###############################################################\n",
    "    #####\n",
    "    #####      assemble the row and put it in output dataframe\n",
    "    #####\n",
    "    ###############################################################\n",
    "    \n",
    "    row = [NassIn_AllFields_AllYears_Acr, NassOut_AllFields_AllYears_Acr,\n",
    "           NassIn_AllFields_CorrectYear_Acr, NassOut_AllFields_CorrectYear_Acr,\n",
    "           NassIn_DoublePoten_AllYears_Acr, NassOut_DoublePoten_AllYears_Acr,\n",
    "           NassIn_DoublePoten_CorrectYear_Acr, NassOut_DoublePoten_CorrectYear_Acr]\n",
    "    output_df.iloc[num, 1: ] = row\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = \"/Users/hn/Desktop/\"\n",
    "filename = write_path + \"Grant_2018_irrigated_acreages_DoublePeaks.csv\"\n",
    "output_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
