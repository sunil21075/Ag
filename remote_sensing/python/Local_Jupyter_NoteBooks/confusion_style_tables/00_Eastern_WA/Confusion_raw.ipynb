{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Grant Irrigated\n",
    "\n",
    "  - In a meeting (May 19th) we decided to drop non-irrigated fields\n",
    "  - I made a mistake for Grant 2018 and did not filter out non-irrigated fields. Hence, we can create statistics related to non-irrigated part. However, we did toss the non-irrigated fields of 2018 for other parts.\n",
    "  - If Min's code is fast, then we do not need to drop anything. I will just run everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "# from shapely.geometry import Point, Polygon\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "import os, os.path\n",
    "import itertools\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "# from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import sys\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "import remote_sensing_core as rc\n",
    "\n",
    "# sys.path.append('/home/hnoorazar/remote_sensing_codes/')\n",
    "# import remote_sensing_core as rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\"\n",
    "\n",
    "double_crop_potens = pd.read_csv(param_dir + \"double_crop_potential_plants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read ShapeFile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grant']\n",
      "['nass' 'wsda' 'producer' 'conservation district']\n"
     ]
    }
   ],
   "source": [
    "shapeFile_Data_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                      \"remote_sensing/01_Data_part_not_filtered/\"\n",
    "\n",
    "WSDA_DataTable_2018 = pd.read_csv(shapeFile_Data_dir + \"WSDA_DataTable_2018.csv\")\n",
    "\n",
    "WSDA_DataTable_2018 = WSDA_DataTable_2018[WSDA_DataTable_2018.county == \"Grant\"]\n",
    "\n",
    "WSDA_DataTable_2018[\"DataSrc\"] = WSDA_DataTable_2018[\"DataSrc\"].str.lower()\n",
    "WSDA_DataTable_2018[\"CropTyp\"] = WSDA_DataTable_2018[\"CropTyp\"].str.lower()\n",
    "\n",
    "print(WSDA_DataTable_2018.county.unique())\n",
    "print(WSDA_DataTable_2018.DataSrc.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeks = \"EVI\"\n",
    "\n",
    "peak_tables_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                  \"remote_sensing/01_NDVI_TS/04_Irrigated_eastern_Cloud70/\" + \\\n",
    "                  \"Grant_2018_irrigated/savitzky_\" + indeks + \"/Grant_Irrigated_2018_no_plot/\"\n",
    "\n",
    "####\n",
    "####  parameters\n",
    "####\n",
    "deltas = [0.1, 0.2, 0.3, 0.4]\n",
    "windows_degrees = [(3, 1), (3, 2),\n",
    "                   (5, 1), (5, 2), (5, 3),\n",
    "                   (7, 1), (7, 2), (7, 3), \n",
    "                   (9, 1), (9, 2), (9, 3)]\n",
    "\n",
    "windows_degrees = [[3, 1], [3, 2],\n",
    "                   [5, 1], [5, 2], [5, 3],\n",
    "                   [7, 1], [7, 2], [7, 3], \n",
    "                   [9, 1], [9, 2], [9, 3]]\n",
    "\n",
    "\n",
    "\n",
    "delta_windows_degrees = [[i, j] for i in deltas for j in windows_degrees]\n",
    "\n",
    "output_columns = ['parameters', 'actual_2_pred_2', 'actual_2_pred_Not2',\n",
    "                  'actual_Not2_pred_2', 'actual_Not2_pred_Not2',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different datafamres based on the following variables\n",
    "\n",
    "- 2 * 2 * 2 * 2 different combinations of NASS, Double_by_Notes, Irrigated, LastSurveyYear\n",
    "\n",
    "For now, leave out the fucking last survey year!\n",
    "\n",
    "In the following we have abbreviated:  \n",
    " - AF: All Fields\n",
    " - DP: Double Potential Fields (i.e. perennials out)\n",
    " - Irr: Just Irrigated Fields\n",
    " - BothIrr: both irrigated and non-irrigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n"
     ]
    }
   ],
   "source": [
    "# NASS_out = False\n",
    "# non_Irr_out = True\n",
    "# perennials_out = False\n",
    "exactly_2_peaks = True\n",
    "\n",
    "for NASS_out in [False]: # True,\n",
    "    for non_Irr_out in [True]: # we have dropped out non-irrigated fields in the peak_finding step\n",
    "        for perennials_out in [True]: # , False\n",
    "            #### \n",
    "            #### build output dataframe\n",
    "            #### \n",
    "            output = pd.DataFrame(data=None, \n",
    "                                  index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                                  columns = output_columns)\n",
    "            output['parameters'] = delta_windows_degrees\n",
    "            \n",
    "            #### \n",
    "            #### Build shapeFile info accordingly\n",
    "            #### \n",
    "            curr_SF = WSDA_DataTable_2018.copy()\n",
    "            \n",
    "            if perennials_out == True:\n",
    "                curr_SF = curr_SF[curr_SF.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                Pere_name = \"PereOut_\"\n",
    "            else:\n",
    "                Pere_name = \"PereIn_\"\n",
    "            \n",
    "#             print(\"perennials_out \" + str(perennials_out))\n",
    "#             print (curr_SF.CropTyp.unique())\n",
    "            \n",
    "            if NASS_out == True:\n",
    "                curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "                NASS_name = \"NASSOut_\"\n",
    "            else:\n",
    "                NASS_name = \"NASSin_\"\n",
    "                \n",
    "            if non_Irr_out == True:\n",
    "                curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "                non_Irr_name = \"JustIrr\"\n",
    "            else:\n",
    "                non_Irr_name = \"BothIrr\"\n",
    " \n",
    "            # print(\"_____________________________________________\")\n",
    "            # print(\"non_Irr_out \" + str(non_Irr_out) + \"curr_SF.Irrigtn:\")\n",
    "            # print (curr_SF.Irrigtn.unique())\n",
    "            \n",
    "            print (\"NASS_out: \" + str(NASS_out) + \", non_Irr_out: \" + str(non_Irr_out) + \\\n",
    "                   \", perennials_out: \" + str(perennials_out))\n",
    "\n",
    "            for location, params in enumerate(output['parameters']):\n",
    "#                 if location % 15 ==0:\n",
    "#                     print (location)\n",
    "                delt = params[0]\n",
    "                win_deg = params[1]\n",
    "                window = win_deg[0]\n",
    "                degree = win_deg[1]\n",
    "                doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                                \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "                doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "                \n",
    "                doubl_peak_table[\"CropTyp\"] = doubl_peak_table[\"CropTyp\"].str.lower()\n",
    "                ############################################################\n",
    "                #\n",
    "                # Drop last row that is empty. We will fix this in the peak_finding step\n",
    "                # Drop the last row\n",
    "                #\n",
    "                ############################################################\n",
    "                last_row_id = doubl_peak_table.shape[0] - 1\n",
    "                doubl_peak_table = doubl_peak_table.drop([last_row_id], axis=0)\n",
    "                \n",
    "                if perennials_out == True:\n",
    "                    doubl_peak_table = doubl_peak_table[\\\n",
    "                                                    doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "#                 print(\"perennials_out \" + str(perennials_out))\n",
    "#                 print (doubl_peak_table.CropTyp.unique())\n",
    "            \n",
    "#                   print (\"No. of rows after dropping perennials \\\n",
    "#                          are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "                            \n",
    "                if NASS_out == True:\n",
    "                    doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "\n",
    "#                 print (\"NASS_out: \" + str(NASS_out))\n",
    "#                 print (\"curr_SF.DataSrc.unique(): \")\n",
    "#                 print (curr_SF.DataSrc.unique())\n",
    "                \n",
    "#                 print (\"doubl_peak_table.DataSrc.unique(): \")\n",
    "#                 print (doubl_peak_table.DataSrc.unique())\n",
    "#                 print(\"____________________________________________________\")\n",
    "\n",
    "                if non_Irr_out == True:\n",
    "                    doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "                # print(\"_____________________________________________\")\n",
    "                # print(\"non_Irr_out \" + str(non_Irr_out) + \" doubl_peak_table.Irrigtn:\")\n",
    "                # print (doubl_peak_table.Irrigtn.unique())\n",
    "\n",
    "                # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "                doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "                doubl_peak_table.drop_duplicates(inplace=True)\n",
    "                # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % \\\n",
    "                # {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                #### \n",
    "                #### Populate output dataframe\n",
    "                #### \n",
    "                actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "                actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "                \n",
    "                if exactly_2_peaks == False:\n",
    "                    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "                    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "                    exactly_2_peaks_name = \"morethan_2peaks\"\n",
    "                else: \n",
    "                    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count == 2].copy()\n",
    "                    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count != 2].copy()\n",
    "                    exactly_2_peaks_name = \"exactly_2_peaks\"\n",
    "\n",
    "                # print (\"There are [%(nrow)d] IDs in curr_SF.\" % \\\n",
    "                #       {\"nrow\":len(curr_SF['ID'])})\n",
    "\n",
    "                # print (\"of which [%(nrow)d] are unique.\" % \\\n",
    "                #        {\"nrow\":len(curr_SF['ID'].unique())})\n",
    "\n",
    "                actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "                actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "                # the follwoing two lines would not work, since we have not \n",
    "                # included the fields with no maximum in our output\n",
    "                # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "                # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "                actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "                actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "                fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "                fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "                output.loc[location, fillin_col] = fillin_vals\n",
    "            \n",
    "            \n",
    "            ###########\n",
    "            output['parameters'] = output['parameters'].astype(\"str\")\n",
    "            write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                         \"remote_sensing/01_NDVI_TS/\" + \\\n",
    "                         \"04_Irrigated_eastern_Cloud70/\" + \\\n",
    "                         \"Grant_2018_irrigated/savitzky_\" + indeks + \"/\"\n",
    "            \n",
    "            os.makedirs(write_path, exist_ok=True)\n",
    "            filename = write_path + \"saviztky_\" + indeks + \"_\" + Pere_name + \\\n",
    "                       NASS_name + non_Irr_name + \"_confusion_\" + exactly_2_peaks_name + \".csv\"\n",
    "            output.to_csv(filename, index = False)\n",
    "            del(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = pd.read_csv(\"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                \"remote_sensing/01_NDVI_TS/04_Irrigated_eastern_Cloud70/\" + \\\n",
    "                \"Grant_2018_irrigated/savitzky_EVI/\" + \n",
    "                \"Grant_Irrigated_2018_no_plot/\" + \\\n",
    "                \"delta0.1_Sav_win3_Order1/all_poly_and_maxs_savitzky.csv\")\n",
    "A.Irrigtn.unique()\n",
    "# drop last row\n",
    "last_row_id = A.shape[0] - 1\n",
    "A = A.drop([last_row_id], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How many of the fields coded with double in Notes are double peakes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curr_SF = WSDA_DataTable_2018.copy()\n",
    "\n",
    "#### build output dataframe\n",
    "output_all_in = pd.DataFrame(data=None, \n",
    "                             index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                             columns = output_columns)\n",
    "\n",
    "output_all_in['parameters'] = delta_windows_degrees\n",
    "\n",
    "# convert index to strings\n",
    "# output_all_in.index = output_all_in.index.map(str)\n",
    "\n",
    "for location, params in enumerate(output_all_in['parameters']):\n",
    "    if location%10 ==0:\n",
    "        print (location)\n",
    "    delt = params[0]\n",
    "    win_deg = params[1]\n",
    "    window = win_deg[0]\n",
    "    degree = win_deg[1]\n",
    "    doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "    doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "    if perennials_out == True:\n",
    "        doubl_peak_table = doubl_peak_table[\\\n",
    "                                        doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "        print (\"No. of rows after dropping perennials are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "    # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "    doubl_peak_table.drop_duplicates(inplace=True)\n",
    "    # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    \n",
    "    #### \n",
    "    #### Populate output dataframe\n",
    "    #### \n",
    "    actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "    actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "\n",
    "    # print (\"There are [%(nrow)d] IDs in curr_SF.\" % \\\n",
    "    #       {\"nrow\":len(curr_SF['ID'])})\n",
    "\n",
    "    # print (\"of which [%(nrow)d] are unique.\" % \\\n",
    "    #        {\"nrow\":len(curr_SF['ID'].unique())})\n",
    "\n",
    "    actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "    actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "    # the follwoing two lines would not work, since we have not \n",
    "    # included the fields with no maximum in our output\n",
    "    # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "    # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "    actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "    actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "    fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "    fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "    output_all_in.loc[location, fillin_col] = fillin_vals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_all_in['parameters'] = output_all_in['parameters'].astype(\"str\")\n",
    "write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "             \"remote_sensing/01_NDVI_TS/\" + \\\n",
    "             \"04_Irrigated_eastern_Cloud70/\" + \\\n",
    "             \"Grant_2018_irrigated/savitzky_EVI/\"\n",
    "\n",
    "os.makedirs(write_path, exist_ok=True)\n",
    "\n",
    "filename = write_path + \"saviztky_EVI_allin_NassIn_BothIrr_AF_confusion_\" + last_name + \".csv\"\n",
    "output_all_in.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = delta_windows_degrees[1]\n",
    "delt = params[0]\n",
    "win_deg = params[1]\n",
    "window = win_deg[0]\n",
    "degree = win_deg[1]\n",
    "\n",
    "doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "doubl_peak_table.head(1)\n",
    "if perennials_out == True:\n",
    "    doubl_peak_table = doubl_peak_table[\\\n",
    "                                    doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "    print (\"No. of rows after dropping perennials are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "doubl_peak_table.drop_duplicates(inplace=True)\n",
    "print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### \n",
    "#### Populate output dataframe\n",
    "#### \n",
    "actual_double_cropped = rc.filter_double_by_Notes(SF_allData_NassIn_BothIrr_AF)\n",
    "actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(SF_allData_NassIn_BothIrr_AF)\n",
    "\n",
    "predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "\n",
    "actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "# the follwoing two lines would not work, since we have not \n",
    "# included the fields with no maximum in our output\n",
    "# actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "# actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "output_all_in.loc[0, fillin_col] = fillin_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How many of the ** <ins>irrigated</ins> ** fields coded with double in Notes are double peakes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curr_SF = WSDA_DataTable_2018.copy()\n",
    "curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "\n",
    "#### build output dataframe\n",
    "output_irrigated = pd.DataFrame(data=None, \n",
    "                             index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                             columns = output_columns)\n",
    "\n",
    "output_irrigated['parameters'] = delta_windows_degrees\n",
    "\n",
    "for location, params in enumerate(output_irrigated['parameters']):\n",
    "    if location%10 ==0:\n",
    "        print (location)\n",
    "    delt = params[0]\n",
    "    win_deg = params[1]\n",
    "    window = win_deg[0]\n",
    "    degree = win_deg[1]\n",
    "    doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "    doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "    \n",
    "    doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "    \n",
    "    if perennials_out == True:\n",
    "        doubl_peak_table = doubl_peak_table[\\\n",
    "                                        doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "        print (\"No. of rows after dropping perennials are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "    # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "    doubl_peak_table.drop_duplicates(inplace=True)\n",
    "    # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    \n",
    "    #### \n",
    "    #### Populate output dataframe\n",
    "    #### \n",
    "    actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "    actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "\n",
    "    actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "    actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "    # the follwoing two lines would not work, since we have not \n",
    "    # included the fields with no maximum in our output\n",
    "    # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "    # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "    actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "    actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "    fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "    fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "    output_irrigated.loc[location, fillin_col] = fillin_vals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_irrigated['parameters'] = output_irrigated['parameters'].astype(\"str\")\n",
    "\n",
    "write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "             \"remote_sensing/01_NDVI_TS/\" + \\\n",
    "             \"04_Irrigated_eastern_Cloud70/\" + \\\n",
    "             \"Grant_2018_irrigated/savitzky_EVI/\"\n",
    "\n",
    "os.makedirs(write_path, exist_ok=True)\n",
    "\n",
    "filename = write_path + \"saviztky_EVI_NassIn_Irr_AF_confusion_\" + last_name + \".csv\"\n",
    "output_irrigated.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How many of the fields coded with double ( ** <ins> by sources other than NASS</ins> **)  in Notes are double peakes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curr_SF = WSDA_DataTable_2018.copy()\n",
    "curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "\n",
    "#### build output dataframe\n",
    "output = pd.DataFrame(data=None, \n",
    "                      index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                      columns = output_columns)\n",
    "\n",
    "output['parameters'] = delta_windows_degrees\n",
    "\n",
    "for location, params in enumerate(output['parameters']):\n",
    "    if location%10 ==0:\n",
    "        print (location)\n",
    "    delt = params[0]\n",
    "    win_deg = params[1]\n",
    "    window = win_deg[0]\n",
    "    degree = win_deg[1]\n",
    "    doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                   \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "    doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "    \n",
    "    doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "    \n",
    "    if perennials_out == True:\n",
    "        doubl_peak_table = doubl_peak_table[\\\n",
    "                                        doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "        print (\"No. of rows after dropping perennials are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "    # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "    doubl_peak_table.drop_duplicates(inplace=True)\n",
    "    # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    \n",
    "    #### \n",
    "    #### Populate output dataframe\n",
    "    #### \n",
    "    actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "    actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "\n",
    "    actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "    actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "    # the follwoing two lines would not work, since we have not \n",
    "    # included the fields with no maximum in our output\n",
    "    # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "    # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "    actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "    actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "    fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "    fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "    output.loc[location, fillin_col] = fillin_vals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output['parameters'] = output['parameters'].astype(\"str\")\n",
    "\n",
    "write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "             \"remote_sensing/01_NDVI_TS/\" + \\\n",
    "             \"04_Irrigated_eastern_Cloud70/\" + \\\n",
    "             \"Grant_2018_irrigated/savitzky_EVI/\"\n",
    "\n",
    "os.makedirs(write_path, exist_ok=True)\n",
    "\n",
    "filename = write_path + \"saviztky_EVI_NassOut_bothIrr_AF_confusion_\" + last_name + \".csv\"\n",
    "output.to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. How many of the ** <ins> irrigated fields </ins>  ** coded with double (** <ins> by sources other than NASS</ins> **) in Notes are double peakes?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curr_SF = WSDA_DataTable_2018.copy()\n",
    "curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "\n",
    "#### build output dataframe\n",
    "output = pd.DataFrame(data=None, \n",
    "                      index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                      columns = output_columns)\n",
    "\n",
    "output['parameters'] = delta_windows_degrees\n",
    "\n",
    "for location, params in enumerate(output['parameters']):\n",
    "    if location%10 ==0:\n",
    "        print (location)\n",
    "    delt = params[0]\n",
    "    win_deg = params[1]\n",
    "    window = win_deg[0]\n",
    "    degree = win_deg[1]\n",
    "    doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                   \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "    doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "    \n",
    "    \n",
    "    doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "    doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "    \n",
    "    if perennials_out == True:\n",
    "        doubl_peak_table = doubl_peak_table[\\\n",
    "                                        doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "\n",
    "    # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "    doubl_peak_table.drop_duplicates(inplace=True)\n",
    "    # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "    \n",
    "    #### \n",
    "    #### Populate output dataframe\n",
    "    #### \n",
    "    actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "    actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "    predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "    predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "\n",
    "    actual_2_pred_2 = sum(actual_double_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "    actual_Not2_pred_2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_double_peaks['ID']))\n",
    "\n",
    "    # the follwoing two lines would not work, since we have not \n",
    "    # included the fields with no maximum in our output\n",
    "    # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "    # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "    actual_2_pred_Not2 = len(actual_double_cropped['ID'].unique()) - actual_2_pred_2\n",
    "    actual_Not2_pred_Not2 = len(actual_Notdouble_cropped['ID'].unique()) - actual_Not2_pred_2\n",
    "\n",
    "    fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "    fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "    output.loc[location, fillin_col] = fillin_vals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output['parameters'] = output['parameters'].astype(\"str\")\n",
    "\n",
    "write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "             \"remote_sensing/01_NDVI_TS/\" + \\\n",
    "             \"04_Irrigated_eastern_Cloud70/\" + \\\n",
    "             \"Grant_2018_irrigated/savitzky_EVI/\"\n",
    "\n",
    "os.makedirs(write_path, exist_ok=True)\n",
    "\n",
    "filename = write_path + \"saviztky_EVI_NassOut_bothIrr_AF_confusion_\" + last_name + \".csv\"\n",
    "output.to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WSDA_DataTable_2018['Irrigtn'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
