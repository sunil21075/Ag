{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Irrigated Fields\n",
    "\n",
    "  - In a meeting (May 19th) we decided to drop non-irrigated fields.\n",
    "  - NASS has to stay in (do not filter it).\n",
    "  - Perennials and Alfalfa has to go out, i.e. Keep those that are potentially double-cropped.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hn/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "# from shapely.geometry import Point, Polygon\n",
    "\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "import os, os.path\n",
    "import itertools\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "# from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import sys\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "sys.path.append('/home/hnoorazar/remote_sensing_codes/')\n",
    "\n",
    "import remote_sensing_core as rc\n",
    "import remote_sensing_core as rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\"\n",
    "\n",
    "double_crop_potens = pd.read_csv(param_dir + \"double_crop_potential_plants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read ShapeFile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may need to write a for-loop if we want more than one county.\n",
    "SF_year = 2017\n",
    "given_county = \"Grant\"\n",
    "indeks = \"EVI\"\n",
    "raw_or_regular = \"regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeFile_Data_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                      \"remote_sensing/01_Data_part_not_filtered/\"\n",
    "\n",
    "WSDA_DataTable = pd.read_csv(shapeFile_Data_dir + \"WSDA_DataTable_\" + str(SF_year) + \".csv\")\n",
    "\n",
    "WSDA_DataTable = WSDA_DataTable[WSDA_DataTable.county == given_county]\n",
    "\n",
    "WSDA_DataTable[\"DataSrc\"] = WSDA_DataTable[\"DataSrc\"].str.lower()\n",
    "WSDA_DataTable[\"CropTyp\"] = WSDA_DataTable[\"CropTyp\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different datafamres based on the following variables\n",
    "\n",
    "- 2 * 2 * 2 * 2 different combinations of NASS, Double_by_Notes, Irrigated, LastSurveyYear\n",
    "\n",
    "For now, leave out the fucking last survey year!\n",
    "\n",
    "In the following we have abbreviated:  \n",
    " - AF: All Fields\n",
    " - DP: Double Potential Fields (i.e. perennials out)\n",
    " - Irr: Just Irrigated Fields\n",
    " - BothIrr: both irrigated and non-irrigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/hn/Documents/01_research_data/remote_sensing/test_aeolus_outputs/\" + \\\n",
    "           \"04_noJump_Regularized_plt_tbl_SOSEOS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside the for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "Grant_2017_regular_EVI_SG_win5_Order1.csv\n",
      "Grant_2017_regular_EVI_SG_win5_Order3.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/hn/Documents/01_research_data/remote_sensing/test_aeolus_outputs/04_noJump_Regularized_plt_tbl_SOSEOS/Grant_2017_regular_EVI_SG_win5_Order3.csv does not exist: '/Users/hn/Documents/01_research_data/remote_sensing/test_aeolus_outputs/04_noJump_Regularized_plt_tbl_SOSEOS/Grant_2017_regular_EVI_SG_win5_Order3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5b30bb82ed4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m                             \u001b[0mdoubl_pk_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                             \u001b[0mdoubl_peak_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoubl_pk_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                             \u001b[0mdoubl_peak_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CropTyp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoubl_peak_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CropTyp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/hn/Documents/01_research_data/remote_sensing/test_aeolus_outputs/04_noJump_Regularized_plt_tbl_SOSEOS/Grant_2017_regular_EVI_SG_win5_Order3.csv does not exist: '/Users/hn/Documents/01_research_data/remote_sensing/test_aeolus_outputs/04_noJump_Regularized_plt_tbl_SOSEOS/Grant_2017_regular_EVI_SG_win5_Order3.csv'"
     ]
    }
   ],
   "source": [
    "for indeks in [\"EVI\"]:\n",
    "\n",
    "    ####\n",
    "    ####  parameters\n",
    "    ####\n",
    "    deltas = [0.1, 0.2, 0.3, 0.4]\n",
    "    delta_windows_degrees = [[5, 1], [5, 3], [7, 3], [9, 3]]\n",
    "\n",
    "    output_columns = ['parameters', 'actual_2_pred_2', 'actual_2_pred_Not2',\n",
    "                      'actual_Not2_pred_2', 'actual_Not2_pred_Not2',]\n",
    "\n",
    "    for exactly_2_peaks in [False, True]:\n",
    "        for double_by_Note in [False]:\n",
    "            for NASS_out in [False]:\n",
    "                # we have dropped out non-irrigated fields in the peak_finding step\n",
    "                for non_Irr_out in [True]: \n",
    "                    for perennials_out in [True]: # \n",
    "                        #### \n",
    "                        #### build output dataframe\n",
    "                        #### \n",
    "                        output = pd.DataFrame(data=None, \n",
    "                                              #delta_windows_degrees\n",
    "                                              index = np.arange(len(delta_windows_degrees)),\n",
    "                                              columns = output_columns)\n",
    "                        output['parameters'] = delta_windows_degrees\n",
    "\n",
    "                        #### \n",
    "                        #### Build shapeFile info accordingly\n",
    "                        #### \n",
    "                        curr_SF = WSDA_DataTable.copy()\n",
    "\n",
    "                        if double_by_Note == False:\n",
    "                            dbl_name = \"_dblNotFiltered_\"\n",
    "                        else:\n",
    "                            curr_SF = rc.filter_double_by_Notes(curr_SF)\n",
    "                            dbl_name = \"_onlyDblByNotes_\"\n",
    "\n",
    "                        if NASS_out == True:\n",
    "                            curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "                            NASS_name = \"NASSOut_\"\n",
    "                        else:\n",
    "                            NASS_name = \"NASSin_\"\n",
    "\n",
    "                        if non_Irr_out == True:\n",
    "                            curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "                            non_Irr_name = \"JustIrr\"\n",
    "                        else:\n",
    "                            non_Irr_name = \"BothIrr\"\n",
    "\n",
    "                        if perennials_out == True:\n",
    "                            curr_SF = curr_SF[curr_SF.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                            Pere_name = \"_PereOut_\"\n",
    "                        else:\n",
    "                            Pere_name = \"_PereIn_\"\n",
    "\n",
    "\n",
    "                        print (\"NASS_out: \" + str(NASS_out) + \", non_Irr_out: \" + str(non_Irr_out) + \\\n",
    "                               \", perennials_out: \" + str(perennials_out))\n",
    "\n",
    "                        for location, params in enumerate(output['parameters']):\n",
    "            #                 if location % 15 ==0:\n",
    "            #                     print (location)\n",
    "                            window = params[0]\n",
    "                            degree = params[1]\n",
    "                            \n",
    "                            f_name = given_county  + \"_\" + str(SF_year) + \"_regular_\" + indeks + \\\n",
    "                                     \"_SG_win\" + str(window) + \"_Order\" + str(degree) + \".csv\"\n",
    "                            print (f_name)\n",
    "                            \n",
    "                            doubl_pk_file = data_dir + f_name\n",
    "\n",
    "                            doubl_peak_table = pd.read_csv(doubl_pk_file, low_memory=False)\n",
    "\n",
    "                            doubl_peak_table[\"CropTyp\"] = doubl_peak_table[\"CropTyp\"].str.lower()\n",
    "\n",
    "                            if double_by_Note == True:\n",
    "                                doubl_peak_table = rc.filter_double_by_Notes(doubl_peak_table)\n",
    "\n",
    "                            if NASS_out == True:\n",
    "                                doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "\n",
    "                             # print (\"NASS_out: \" + str(NASS_out))\n",
    "                             # print (\"curr_SF.DataSrc.unique(): \")\n",
    "                             # print (curr_SF.DataSrc.unique())\n",
    "\n",
    "                             # print (\"doubl_peak_table.DataSrc.unique(): \")\n",
    "                             # print (doubl_peak_table.DataSrc.unique())\n",
    "                             # print(\"____________________________________________________\")\n",
    "\n",
    "                            if non_Irr_out == True:\n",
    "                                doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "                            # print(\"_____________________________________________\")\n",
    "                            # print(\"non_Irr_out \" + str(non_Irr_out) + \" doubl_peak_table.Irrigtn:\")\n",
    "                            # print (doubl_peak_table.Irrigtn.unique())\n",
    "\n",
    "                            if perennials_out == True:\n",
    "                                doubl_peak_table = doubl_peak_table[\\\n",
    "                                         doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                             # print(\"perennials_out \" + str(perennials_out))\n",
    "                             # print (doubl_peak_table.CropTyp.unique())\n",
    "\n",
    "                             # print (\"No. of rows after dropping perennials \\\n",
    "                             #          are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                            # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "                            doubl_peak_table.drop(['doy', 'EVI', 'system_start_time', 'Date', \n",
    "                                                   'human_system_start_time', \n",
    "                                                   'EVI_ratio','SOS', 'EOS'], axis=1, inplace=True)\n",
    "                            \n",
    "                            doubl_peak_table.drop_duplicates(inplace=True)\n",
    "                            # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % \\\n",
    "                            # {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                            #### \n",
    "                            #### Populate output dataframe\n",
    "                            #### \n",
    "                            actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "                            actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "                            if exactly_2_peaks == False:\n",
    "                                predicted_double_peaks = doubl_peak_table[\\\n",
    "                                                         doubl_peak_table.season_count >= 2].copy()\n",
    "                                \n",
    "                                predicted_Notdouble_peaks = doubl_peak_table[\\\n",
    "                                                            doubl_peak_table.season_count < 2].copy()\n",
    "                                exactly_2_peaks_name = \"morethan2Peaks\"\n",
    "                            else:\n",
    "                                predicted_double_peaks = doubl_peak_table[\\\n",
    "                                                                    doubl_peak_table.season_count == 2].copy()\n",
    "                                \n",
    "                                \n",
    "                                predicted_Notdouble_peaks = doubl_peak_table[\\\n",
    "                                                            doubl_peak_table.season_count != 2].copy()\n",
    "                                \n",
    "                                exactly_2_peaks_name = \"exactly2Peaks\"\n",
    "\n",
    "                            # print (\"There are [%(nrow)d] IDs in curr_SF.\" % \\\n",
    "                            #       {\"nrow\":len(curr_SF['ID'])})\n",
    "\n",
    "                            # print (\"of which [%(nrow)d] are unique.\" % \\\n",
    "                            #        {\"nrow\":len(curr_SF['ID'].unique())})\n",
    "\n",
    "                            actual_2_pred_2 = actual_double_cropped[\\\n",
    "                                            actual_double_cropped['ID'].isin(predicted_double_peaks['ID'])]\n",
    "\n",
    "                            actual_2_pred_2 = actual_2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                            actual_Not2_pred_2 = actual_Notdouble_cropped[\\\n",
    "                                                   actual_Notdouble_cropped['ID'].isin(\\\n",
    "                                                                  predicted_double_peaks['ID'])]\n",
    "                            actual_Not2_pred_2 = actual_Not2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                            # the follwoing two lines would not work, since we have not \n",
    "                            # included the fields with no maximum in our output\n",
    "                            # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(\\\n",
    "                            #                                  predicted_Notdouble_peaks['ID']))\n",
    "                            # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(\\\n",
    "                            #                            predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "                            actual_2_pred_Not2 = actual_double_cropped['ExctAcr'].sum() - actual_2_pred_2\n",
    "\n",
    "                            actual_Not2_pred_Not2 = actual_Notdouble_cropped['ExctAcr'].sum() - \\\n",
    "                                                      actual_Not2_pred_2\n",
    "\n",
    "                            fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \\\n",
    "                                          \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "\n",
    "                            fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, \\\n",
    "                                           actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "\n",
    "                            output.loc[location, fillin_col] = fillin_vals\n",
    "\n",
    "\n",
    "                        ###########\n",
    "                        output['parameters'] = output['parameters'].astype(\"str\")\n",
    "                        write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                                     \"remote_sensing/01_NDVI_TS/00_Eastern_WA_withYear/2Years/\" + \\\n",
    "                                     \"2Yrs_tables_\" + raw_or_regular + \"/\"\n",
    "                                      \n",
    "\n",
    "                        os.makedirs(write_path, exist_ok=True)\n",
    "                        filename = write_path + given_county + \"_\" + str(SF_year) + \"_\" + indeks + \\\n",
    "                                   Pere_name + NASS_name + non_Irr_name + dbl_name + \\\n",
    "                                   \"confusion_Acr_\" + exactly_2_peaks_name + \"_\" + \\\n",
    "                                    raw_or_regular + \".csv\"\n",
    "\n",
    "\n",
    "                        output['actual_2_pred_2'] = output['actual_2_pred_2'].astype(float)\n",
    "                        output['actual_2_pred_Not2'] = output['actual_2_pred_Not2'].astype(float)\n",
    "                        output['actual_Not2_pred_2'] = output['actual_Not2_pred_2'].astype(float)\n",
    "                        output['actual_Not2_pred_Not2'] = output['actual_Not2_pred_Not2'].astype(float)\n",
    "                        output = output.round(decimals=2)\n",
    "\n",
    "                        output.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>actual_2_pred_2</th>\n",
       "      <th>actual_2_pred_Not2</th>\n",
       "      <th>actual_Not2_pred_2</th>\n",
       "      <th>actual_Not2_pred_Not2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>4486.66</td>\n",
       "      <td>11924.5</td>\n",
       "      <td>29788.9</td>\n",
       "      <td>210114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5, 3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[7, 3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, 3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parameters actual_2_pred_2 actual_2_pred_Not2 actual_Not2_pred_2  \\\n",
       "0     [5, 1]         4486.66            11924.5            29788.9   \n",
       "1     [5, 3]             NaN                NaN                NaN   \n",
       "2     [7, 3]             NaN                NaN                NaN   \n",
       "3     [9, 3]             NaN                NaN                NaN   \n",
       "\n",
       "  actual_Not2_pred_Not2  \n",
       "0                210114  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4575"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doubl_peak_table.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
