{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Irrigated Fields\n",
    "\n",
    "  - In a meeting (May 19th) we decided to drop non-irrigated fields.\n",
    "  - NASS has to stay in (do not filter it).\n",
    "  - Perennials and Alfalfa has to go out, i.e. Keep those that are potentially double-cropped.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "# from shapely.geometry import Point, Polygon\n",
    "\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "import os, os.path\n",
    "import itertools\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "# from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import sys\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "import remote_sensing_core as rc\n",
    "\n",
    "# sys.path.append('/home/hnoorazar/remote_sensing_codes/')\n",
    "# import remote_sensing_core as rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\"\n",
    "\n",
    "double_crop_potens = pd.read_csv(param_dir + \"double_crop_potential_plants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read ShapeFile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may need to write a for-loop if we want more than one county.\n",
    "given_county = \"Grant\"\n",
    "indeks = \"EVI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeFile_Data_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                      \"remote_sensing/01_Data_part_not_filtered/\"\n",
    "\n",
    "WSDA_DataTable = pd.read_csv(shapeFile_Data_dir + \"WSDA_DataTable_\" + str(SF_year) + \".csv\")\n",
    "\n",
    "WSDA_DataTable = WSDA_DataTable[WSDA_DataTable.county == given_county]\n",
    "\n",
    "WSDA_DataTable[\"DataSrc\"] = WSDA_DataTable[\"DataSrc\"].str.lower()\n",
    "WSDA_DataTable[\"CropTyp\"] = WSDA_DataTable[\"CropTyp\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_tables_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                  \"remote_sensing/01_NDVI_TS/00_Eastern_WA_withYear/tables/\" + \\\n",
    "                   given_county + \"_\" + str(SF_year) + \"/savitzky_\" + indeks + \"/\"\n",
    "\n",
    "####\n",
    "####  parameters\n",
    "####\n",
    "deltas = [0.1, 0.2, 0.3, 0.4]\n",
    "windows_degrees = [(3, 1), (3, 2),\n",
    "                   (5, 1), (5, 2), (5, 3),\n",
    "                   (7, 1), (7, 2), (7, 3), \n",
    "                   (9, 1), (9, 2), (9, 3)]\n",
    "\n",
    "windows_degrees = [[3, 1], [3, 2],\n",
    "                   [5, 1], [5, 2], [5, 3],\n",
    "                   [7, 1], [7, 2], [7, 3], \n",
    "                   [9, 1], [9, 2], [9, 3]]\n",
    "\n",
    "\n",
    "\n",
    "delta_windows_degrees = [[i, j] for i in deltas for j in windows_degrees]\n",
    "\n",
    "output_columns = ['parameters', 'actual_2_pred_2', 'actual_2_pred_Not2',\n",
    "                  'actual_Not2_pred_2', 'actual_Not2_pred_Not2',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different datafamres based on the following variables\n",
    "\n",
    "- 2 * 2 * 2 * 2 different combinations of NASS, Double_by_Notes, Irrigated, LastSurveyYear\n",
    "\n",
    "For now, leave out the fucking last survey year!\n",
    "\n",
    "In the following we have abbreviated:  \n",
    " - AF: All Fields\n",
    " - DP: Double Potential Fields (i.e. perennials out)\n",
    " - Irr: Just Irrigated Fields\n",
    " - BothIrr: both irrigated and non-irrigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n"
     ]
    }
   ],
   "source": [
    "# NASS_out = False\n",
    "# non_Irr_out = True\n",
    "# perennials_out = True\n",
    "exactly_2_peaks = False\n",
    "double_by_Note = False\n",
    "\n",
    "for exactly_2_peaks in [False, True]:\n",
    "    for double_by_Note in [False, True]:\n",
    "        for NASS_out in [False]:\n",
    "            for non_Irr_out in [True]: # we have dropped out non-irrigated fields in the peak_finding step\n",
    "                for perennials_out in [True]: # \n",
    "                    #### \n",
    "                    #### build output dataframe\n",
    "                    #### \n",
    "                    output = pd.DataFrame(data=None, \n",
    "                                          index = np.arange(len(delta_windows_degrees)), # delta_windows_degrees, # \n",
    "                                          columns = output_columns)\n",
    "                    output['parameters'] = delta_windows_degrees\n",
    "\n",
    "                    #### \n",
    "                    #### Build shapeFile info accordingly\n",
    "                    #### \n",
    "                    curr_SF = WSDA_DataTable.copy()\n",
    "                    \n",
    "                    if double_by_Note == False:\n",
    "                        dbl_name = \"_dblNotFiltered_\"\n",
    "                    else:\n",
    "                        curr_SF = rc.filter_double_by_Notes(curr_SF)\n",
    "                        dbl_name = \"_onlyDblByNotes_\"\n",
    "                    \n",
    "                    if NASS_out == True:\n",
    "                        curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "                        NASS_name = \"NASSOut_\"\n",
    "                    else:\n",
    "                        NASS_name = \"NASSin_\"\n",
    "\n",
    "                    if non_Irr_out == True:\n",
    "                        curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "                        non_Irr_name = \"JustIrr\"\n",
    "                    else:\n",
    "                        non_Irr_name = \"BothIrr\"\n",
    "                        \n",
    "                    if perennials_out == True:\n",
    "                        curr_SF = curr_SF[curr_SF.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                        Pere_name = \"_PereOut_\"\n",
    "                    else:\n",
    "                        Pere_name = \"_PereIn_\"\n",
    "\n",
    "\n",
    "                    print (\"NASS_out: \" + str(NASS_out) + \", non_Irr_out: \" + str(non_Irr_out) + \\\n",
    "                           \", perennials_out: \" + str(perennials_out))\n",
    "\n",
    "                    for location, params in enumerate(output['parameters']):\n",
    "        #                 if location % 15 ==0:\n",
    "        #                     print (location)\n",
    "                        delt = params[0]\n",
    "                        win_deg = params[1]\n",
    "                        window = win_deg[0]\n",
    "                        degree = win_deg[1]\n",
    "                        doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \"_Sav_win\" + str(window) + \\\n",
    "                                        \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "                        doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "\n",
    "                        doubl_peak_table[\"CropTyp\"] = doubl_peak_table[\"CropTyp\"].str.lower()\n",
    "                        ############################################################\n",
    "                        #\n",
    "                        # Drop last row that is empty. We will fix this in the peak_finding step\n",
    "                        # Drop the last row\n",
    "                        #\n",
    "                        ############################################################\n",
    "                        last_row_id = doubl_peak_table.shape[0] - 1\n",
    "                        doubl_peak_table = doubl_peak_table.drop([last_row_id], axis=0)\n",
    "\n",
    "                        if double_by_Note == True:\n",
    "                            doubl_peak_table = rc.filter_double_by_Notes(doubl_peak_table)\n",
    "\n",
    "                        if NASS_out == True:\n",
    "                            doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "\n",
    "                         # print (\"NASS_out: \" + str(NASS_out))\n",
    "                         # print (\"curr_SF.DataSrc.unique(): \")\n",
    "                         # print (curr_SF.DataSrc.unique())\n",
    "\n",
    "                         # print (\"doubl_peak_table.DataSrc.unique(): \")\n",
    "                         # print (doubl_peak_table.DataSrc.unique())\n",
    "                         # print(\"____________________________________________________\")\n",
    "\n",
    "                        if non_Irr_out == True:\n",
    "                            doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "                        # print(\"_____________________________________________\")\n",
    "                        # print(\"non_Irr_out \" + str(non_Irr_out) + \" doubl_peak_table.Irrigtn:\")\n",
    "                        # print (doubl_peak_table.Irrigtn.unique())\n",
    "                        \n",
    "                        if perennials_out == True:\n",
    "                            doubl_peak_table = doubl_peak_table[\\\n",
    "                                                doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                         # print(\"perennials_out \" + str(perennials_out))\n",
    "                         # print (doubl_peak_table.CropTyp.unique())\n",
    "\n",
    "                         # print (\"No. of rows after dropping perennials \\\n",
    "                         #          are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                        # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "                        doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "                        doubl_peak_table.drop_duplicates(inplace=True)\n",
    "                        # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % \\\n",
    "                        # {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                        #### \n",
    "                        #### Populate output dataframe\n",
    "                        #### \n",
    "                        actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "                        actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "                        if exactly_2_peaks == False:\n",
    "                            predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count >= 2].copy()\n",
    "                            predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count < 2].copy()\n",
    "                            exactly_2_peaks_name = \"morethan2Peaks\"\n",
    "                        else:\n",
    "                            predicted_double_peaks = doubl_peak_table[doubl_peak_table.max_count == 2].copy()\n",
    "                            predicted_Notdouble_peaks = doubl_peak_table[doubl_peak_table.max_count != 2].copy()\n",
    "                            exactly_2_peaks_name = \"exactly2Peaks\"\n",
    "\n",
    "                        # print (\"There are [%(nrow)d] IDs in curr_SF.\" % \\\n",
    "                        #       {\"nrow\":len(curr_SF['ID'])})\n",
    "\n",
    "                        # print (\"of which [%(nrow)d] are unique.\" % \\\n",
    "                        #        {\"nrow\":len(curr_SF['ID'].unique())})\n",
    "\n",
    "                        actual_2_pred_2 = actual_double_cropped[\\\n",
    "                                                actual_double_cropped['ID'].isin(predicted_double_peaks['ID'])]\n",
    "\n",
    "                        actual_2_pred_2 = actual_2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                        actual_Not2_pred_2 = actual_Notdouble_cropped[\\\n",
    "                                                       actual_Notdouble_cropped['ID'].isin(\\\n",
    "                                                                                  predicted_double_peaks['ID'])]\n",
    "                        actual_Not2_pred_2 = actual_Not2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                        # the follwoing two lines would not work, since we have not \n",
    "                        # included the fields with no maximum in our output\n",
    "                        # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(\\\n",
    "                        #                                  predicted_Notdouble_peaks['ID']))\n",
    "                        # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(\\\n",
    "                        #                            predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "                        actual_2_pred_Not2 = actual_double_cropped['ExctAcr'].sum() - actual_2_pred_2\n",
    "                        actual_Not2_pred_Not2 = actual_Notdouble_cropped['ExctAcr'].sum() - actual_Not2_pred_2\n",
    "\n",
    "                        fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \\\n",
    "                                      \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "\n",
    "                        fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, \\\n",
    "                                       actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "                        \n",
    "                        output.loc[location, fillin_col] = fillin_vals\n",
    "\n",
    "\n",
    "                    ###########\n",
    "                    output['parameters'] = output['parameters'].astype(\"str\")\n",
    "                    write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                                 \"remote_sensing/01_NDVI_TS/00_Eastern_WA_withYear/tables/\" + \\\n",
    "                                  given_county + \"_\" + str(SF_year) + \"/\"\n",
    "\n",
    "                    os.makedirs(write_path, exist_ok=True)\n",
    "                    filename = write_path + \"SG_\" + indeks + \\\n",
    "                               Pere_name + NASS_name + non_Irr_name + dbl_name + \\\n",
    "                               \"confusion_Acr_\" + exactly_2_peaks_name + \".csv\"\n",
    "\n",
    "\n",
    "                    output['actual_2_pred_2'] = output['actual_2_pred_2'].astype(float)\n",
    "                    output['actual_2_pred_Not2'] = output['actual_2_pred_Not2'].astype(float)\n",
    "                    output['actual_Not2_pred_2'] = output['actual_Not2_pred_2'].astype(float)\n",
    "                    output['actual_Not2_pred_Not2'] = output['actual_Not2_pred_Not2'].astype(float)\n",
    "                    output = output.round(decimals=2)\n",
    "\n",
    "                    output.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
