{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for Irrigated Fields\n",
    "\n",
    "  - In a meeting (May 19th) we decided to drop non-irrigated fields.\n",
    "  - NASS has to stay in (do not filter it).\n",
    "  - Perennials and Alfalfa has to go out, i.e. Keep those that are potentially double-cropped.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import geopandas as gpd\n",
    "from IPython.display import Image\n",
    "# from shapely.geometry import Point, Polygon\n",
    "\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "import os, os.path\n",
    "import itertools\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "# from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import sys\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "sys.path.append('/home/hnoorazar/remote_sensing_codes/')\n",
    "\n",
    "import remote_sensing_core as rc\n",
    "import remote_sensing_core as rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\"\n",
    "\n",
    "double_crop_potens = pd.read_csv(param_dir + \"double_crop_potential_plants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read ShapeFile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may need to write a for-loop if we want more than one county.\n",
    "SF_year = 2017\n",
    "given_county = \"Grant\"\n",
    "indeks = \"NDVI\"\n",
    "raw_or_regular = \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeFile_Data_dir = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                      \"remote_sensing/01_Data_part_not_filtered/\"\n",
    "\n",
    "WSDA_DataTable = pd.read_csv(shapeFile_Data_dir + \"WSDA_DataTable_\" + str(SF_year) + \".csv\")\n",
    "\n",
    "WSDA_DataTable = WSDA_DataTable[WSDA_DataTable.county == given_county]\n",
    "\n",
    "WSDA_DataTable[\"DataSrc\"] = WSDA_DataTable[\"DataSrc\"].str.lower()\n",
    "WSDA_DataTable[\"CropTyp\"] = WSDA_DataTable[\"CropTyp\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different datafamres based on the following variables\n",
    "\n",
    "- 2 * 2 * 2 * 2 different combinations of NASS, Double_by_Notes, Irrigated, LastSurveyYear\n",
    "\n",
    "For now, leave out the fucking last survey year!\n",
    "\n",
    "In the following we have abbreviated:  \n",
    " - AF: All Fields\n",
    " - DP: Double Potential Fields (i.e. perennials out)\n",
    " - Irr: Just Irrigated Fields\n",
    " - BothIrr: both irrigated and non-irrigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n",
      "NASS_out: False, non_Irr_out: True, perennials_out: True\n"
     ]
    }
   ],
   "source": [
    "for indeks in [\"EVI\", \"NDVI\"]:\n",
    "    peak_tables_dir = \"/Users/hn/Documents/01_research_data/remote_sensing/\" + \\\n",
    "                      \"01_NDVI_TS/00_Eastern_WA_withYear/2Years/2Yrs_tables_\" + \\\n",
    "                       raw_or_regular + \"/\" + given_county + \"_\" + str(SF_year) + \\\n",
    "                       \"_\" + raw_or_regular + \"_savitzky_\" + indeks + \"/\"\n",
    "\n",
    "    ####\n",
    "    ####  parameters\n",
    "    ####\n",
    "    deltas = [0.1, 0.2, 0.3, 0.4]\n",
    "    windows_degrees = [(3, 1), (3, 2),\n",
    "                       (5, 1), (5, 2), (5, 3),\n",
    "                       (7, 1), (7, 2), (7, 3), \n",
    "                       (9, 1), (9, 2), (9, 3)]\n",
    "\n",
    "    windows_degrees = [[3, 1], [3, 2],\n",
    "                       [5, 1], [5, 2], [5, 3],\n",
    "                       [7, 1], [7, 2], [7, 3], \n",
    "                       [9, 1], [9, 2], [9, 3]]\n",
    "\n",
    "    delta_windows_degrees = [[i, j] for i in deltas for j in windows_degrees]\n",
    "\n",
    "    output_columns = ['parameters', 'actual_2_pred_2', 'actual_2_pred_Not2',\n",
    "                      'actual_Not2_pred_2', 'actual_Not2_pred_Not2',]\n",
    "\n",
    "    # NASS_out = False\n",
    "    # non_Irr_out = True\n",
    "    # perennials_out = True\n",
    "    exactly_2_peaks = False\n",
    "    double_by_Note = False\n",
    "\n",
    "    for exactly_2_peaks in [False, True]:\n",
    "        for double_by_Note in [False]:\n",
    "            for NASS_out in [False]:\n",
    "                # we have dropped out non-irrigated fields in the peak_finding step\n",
    "                for non_Irr_out in [True]: \n",
    "                    for perennials_out in [True]: # \n",
    "                        #### \n",
    "                        #### build output dataframe\n",
    "                        #### \n",
    "                        output = pd.DataFrame(data=None, \n",
    "                                              #delta_windows_degrees\n",
    "                                              index = np.arange(len(delta_windows_degrees)),\n",
    "                                              columns = output_columns)\n",
    "                        output['parameters'] = delta_windows_degrees\n",
    "\n",
    "                        #### \n",
    "                        #### Build shapeFile info accordingly\n",
    "                        #### \n",
    "                        curr_SF = WSDA_DataTable.copy()\n",
    "\n",
    "                        if double_by_Note == False:\n",
    "                            dbl_name = \"_dblNotFiltered_\"\n",
    "                        else:\n",
    "                            curr_SF = rc.filter_double_by_Notes(curr_SF)\n",
    "                            dbl_name = \"_onlyDblByNotes_\"\n",
    "\n",
    "                        if NASS_out == True:\n",
    "                            curr_SF = rc.filter_out_NASS(curr_SF)\n",
    "                            NASS_name = \"NASSOut_\"\n",
    "                        else:\n",
    "                            NASS_name = \"NASSin_\"\n",
    "\n",
    "                        if non_Irr_out == True:\n",
    "                            curr_SF = rc.filter_out_nonIrrigated(curr_SF)\n",
    "                            non_Irr_name = \"JustIrr\"\n",
    "                        else:\n",
    "                            non_Irr_name = \"BothIrr\"\n",
    "\n",
    "                        if perennials_out == True:\n",
    "                            curr_SF = curr_SF[curr_SF.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                            Pere_name = \"_PereOut_\"\n",
    "                        else:\n",
    "                            Pere_name = \"_PereIn_\"\n",
    "\n",
    "\n",
    "                        print (\"NASS_out: \" + str(NASS_out) + \", non_Irr_out: \" + str(non_Irr_out) + \\\n",
    "                               \", perennials_out: \" + str(perennials_out))\n",
    "\n",
    "                        for location, params in enumerate(output['parameters']):\n",
    "            #                 if location % 15 ==0:\n",
    "            #                     print (location)\n",
    "                            delt = params[0]\n",
    "                            win_deg = params[1]\n",
    "                            window = win_deg[0]\n",
    "                            degree = win_deg[1]\n",
    "                            doubl_pk_file = peak_tables_dir + \"delta\" + str(delt) + \\\n",
    "                                            \"_Sav_win\" + str(window) + \\\n",
    "                                            \"_Order\" + str(degree) + \"/all_poly_and_maxs_savitzky.csv\"\n",
    "\n",
    "                            doubl_peak_table = pd.read_csv( doubl_pk_file, low_memory=False)\n",
    "\n",
    "                            doubl_peak_table[\"CropTyp\"] = doubl_peak_table[\"CropTyp\"].str.lower()\n",
    "                            ############################################################\n",
    "                            #\n",
    "                            # Drop last row that is empty. We will fix this in the peak_finding step\n",
    "                            # Drop the last row\n",
    "                            #\n",
    "                            ############################################################\n",
    "                            last_row_id = doubl_peak_table.shape[0] - 1\n",
    "                            doubl_peak_table = doubl_peak_table.drop([last_row_id], axis=0)\n",
    "\n",
    "                            if double_by_Note == True:\n",
    "                                doubl_peak_table = rc.filter_double_by_Notes(doubl_peak_table)\n",
    "\n",
    "                            if NASS_out == True:\n",
    "                                doubl_peak_table = rc.filter_out_NASS(doubl_peak_table)\n",
    "\n",
    "                             # print (\"NASS_out: \" + str(NASS_out))\n",
    "                             # print (\"curr_SF.DataSrc.unique(): \")\n",
    "                             # print (curr_SF.DataSrc.unique())\n",
    "\n",
    "                             # print (\"doubl_peak_table.DataSrc.unique(): \")\n",
    "                             # print (doubl_peak_table.DataSrc.unique())\n",
    "                             # print(\"____________________________________________________\")\n",
    "\n",
    "                            if non_Irr_out == True:\n",
    "                                doubl_peak_table = rc.filter_out_nonIrrigated(doubl_peak_table)\n",
    "                            # print(\"_____________________________________________\")\n",
    "                            # print(\"non_Irr_out \" + str(non_Irr_out) + \" doubl_peak_table.Irrigtn:\")\n",
    "                            # print (doubl_peak_table.Irrigtn.unique())\n",
    "\n",
    "                            if perennials_out == True:\n",
    "                                doubl_peak_table = doubl_peak_table[\\\n",
    "                                         doubl_peak_table.CropTyp.isin(double_crop_potens['Crop_Type'])]\n",
    "                             # print(\"perennials_out \" + str(perennials_out))\n",
    "                             # print (doubl_peak_table.CropTyp.unique())\n",
    "\n",
    "                             # print (\"No. of rows after dropping perennials \\\n",
    "                             #          are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                            # print (\"No. of rows are [%(nrow)d].\" % {\"nrow\":doubl_peak_table.shape[0]})\n",
    "                            doubl_peak_table.drop(['max_Doy', 'max_value'], axis=1, inplace=True)\n",
    "                            doubl_peak_table.drop_duplicates(inplace=True)\n",
    "                            # print (\"No. of rows after dropping dupliates are [%(nrow)d].\" % \\\n",
    "                            # {\"nrow\":doubl_peak_table.shape[0]})\n",
    "\n",
    "                            #### \n",
    "                            #### Populate output dataframe\n",
    "                            #### \n",
    "                            actual_double_cropped = rc.filter_double_by_Notes(curr_SF)\n",
    "                            actual_Notdouble_cropped = rc.filter_Notdouble_by_Notes(curr_SF)\n",
    "\n",
    "                            if exactly_2_peaks == False:\n",
    "                                predicted_double_peaks = doubl_peak_table[\\\n",
    "                                                         doubl_peak_table.max_count >= 2].copy()\n",
    "                                \n",
    "                                predicted_Notdouble_peaks = doubl_peak_table[\\\n",
    "                                                            doubl_peak_table.max_count < 2].copy()\n",
    "                                exactly_2_peaks_name = \"morethan2Peaks\"\n",
    "                            else:\n",
    "                                predicted_double_peaks = doubl_peak_table[\\\n",
    "                                                                    doubl_peak_table.max_count == 2].copy()\n",
    "                                \n",
    "                                \n",
    "                                predicted_Notdouble_peaks = doubl_peak_table[\\\n",
    "                                                            doubl_peak_table.max_count != 2].copy()\n",
    "                                \n",
    "                                exactly_2_peaks_name = \"exactly2Peaks\"\n",
    "\n",
    "                            # print (\"There are [%(nrow)d] IDs in curr_SF.\" % \\\n",
    "                            #       {\"nrow\":len(curr_SF['ID'])})\n",
    "\n",
    "                            # print (\"of which [%(nrow)d] are unique.\" % \\\n",
    "                            #        {\"nrow\":len(curr_SF['ID'].unique())})\n",
    "\n",
    "                            actual_2_pred_2 = actual_double_cropped[\\\n",
    "                                            actual_double_cropped['ID'].isin(predicted_double_peaks['ID'])]\n",
    "\n",
    "                            actual_2_pred_2 = actual_2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                            actual_Not2_pred_2 = actual_Notdouble_cropped[\\\n",
    "                                                   actual_Notdouble_cropped['ID'].isin(\\\n",
    "                                                                  predicted_double_peaks['ID'])]\n",
    "                            actual_Not2_pred_2 = actual_Not2_pred_2['ExctAcr'].sum()\n",
    "\n",
    "                            # the follwoing two lines would not work, since we have not \n",
    "                            # included the fields with no maximum in our output\n",
    "                            # actual_2_pred_Not2 = sum(actual_double_cropped['ID'].isin(\\\n",
    "                            #                                  predicted_Notdouble_peaks['ID']))\n",
    "                            # actual_Not2_pred_Not2 = sum(actual_Notdouble_cropped['ID'].isin(\\\n",
    "                            #                            predicted_Notdouble_peaks['ID']))\n",
    "\n",
    "                            actual_2_pred_Not2 = actual_double_cropped['ExctAcr'].sum() - actual_2_pred_2\n",
    "\n",
    "                            actual_Not2_pred_Not2 = actual_Notdouble_cropped['ExctAcr'].sum() - \\\n",
    "                                                      actual_Not2_pred_2\n",
    "\n",
    "                            fillin_col = [\"actual_2_pred_2\", \"actual_2_pred_Not2\", \\\n",
    "                                          \"actual_Not2_pred_2\", \"actual_Not2_pred_Not2\"]\n",
    "\n",
    "                            fillin_vals = [actual_2_pred_2, actual_2_pred_Not2, \\\n",
    "                                           actual_Not2_pred_2, actual_Not2_pred_Not2]\n",
    "\n",
    "                            output.loc[location, fillin_col] = fillin_vals\n",
    "\n",
    "\n",
    "                        ###########\n",
    "                        output['parameters'] = output['parameters'].astype(\"str\")\n",
    "                        write_path = \"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                                     \"remote_sensing/01_NDVI_TS/00_Eastern_WA_withYear/2Years/\" + \\\n",
    "                                     \"2Yrs_tables_\" + raw_or_regular + \"/\"\n",
    "                                      \n",
    "\n",
    "                        os.makedirs(write_path, exist_ok=True)\n",
    "                        filename = write_path + given_county + \"_\" + str(SF_year) + \"_\" + indeks + \\\n",
    "                                   Pere_name + NASS_name + non_Irr_name + dbl_name + \\\n",
    "                                   \"confusion_Acr_\" + exactly_2_peaks_name + \"_\" + \\\n",
    "                                    raw_or_regular + \".csv\"\n",
    "\n",
    "\n",
    "                        output['actual_2_pred_2'] = output['actual_2_pred_2'].astype(float)\n",
    "                        output['actual_2_pred_Not2'] = output['actual_2_pred_Not2'].astype(float)\n",
    "                        output['actual_Not2_pred_2'] = output['actual_Not2_pred_2'].astype(float)\n",
    "                        output['actual_Not2_pred_Not2'] = output['actual_Not2_pred_Not2'].astype(float)\n",
    "                        output = output.round(decimals=2)\n",
    "\n",
    "                        output.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
